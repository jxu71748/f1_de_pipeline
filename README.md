# Formula 1 Data Pipeline Project ğŸï¸ ğŸï¸ ğŸï¸ 

## Project Objective
This project is part of the DE Zoomcamp course final project. The goal is to build a complete batch data pipeline using GCP, Terraform, Airflow, and Spark. The dataset is from Kaggle and contains Formula 1 race data from 1950 to 2024.

## Tools Used
- Cloud: Google Cloud Platform (GCS & BigQuery)
- Infrastructure as code (IaC): Terraform for infrastructure provisioning
- Workflow orchestration: Airflow
- Batch processing: PySpark for data transformation
- Visualization: Google Data Studio
- Python for scripting (data download, upload, transformation)

## Current Progress
- âœ… Downloaded raw data from Kaggle
- âœ… Created GCS bucket and BigQuery dataset using Terraform
- ğŸ”œ Upload raw CSV files to GCS via Python
- ğŸ”œ Build Airflow DAGs for ingestion and transformation
- ğŸ”œ Create a dashboard with race results insights

## Folder Structure

